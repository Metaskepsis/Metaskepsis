{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from simple_workflows import *\n",
    "from simple_tools import *\n",
    "from workflows_as_tools import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receptionist: The following has been forwarded to the arxiv_retriever:  I'll create queries for each paper and send them to the retriever.\n",
      "\n",
      "Here's the first query:\n",
      "\n",
      "`\"An interpolating distance between optimal transport and Fisher-Rao metrics\"`\n",
      "\n",
      "Please wait for the response before I proceed to the next query.\n",
      "Retriever: I am going to call  get_id_from_url\n",
      "Tool_executor: I am going to executeget_id_from_urlwith{'url': 'https://export.arxiv.org/api/query?search_query=An_interpolating_distance_between_optimal_transport_and_Fisher-Rao_metrics&max_results=5'}\n",
      "Scraper: I got an error, going back to the arxiv_retriever\n",
      "Retriever:I am reporting back to the arxiv_receptionist withNo paper found for the query. Let's try with a different query.\n",
      "Reporting to receptionist\n",
      "Receptionist: The following has been forwarded to the arxiv_retriever:  Here's a revised query:\n",
      "\n",
      "`\"interpolating distance between optimal transport Fisher-Rao metrics\"`\n",
      "\n",
      "Please wait for the response.\n",
      "Retriever: I am going to call  get_id_from_url\n",
      "Tool_executor: I am going to executeget_id_from_urlwith{'url': 'https://export.arxiv.org/api/query?search_query=interpolating+distance+between+optimal+transport+Fisher-Rao+metrics&max_results=5'}\n",
      "Scraper: I got the following paperThe most relevant arXiv paper to the query is \"An Interpolating Distance between Optimal Transport and Fisher-Rao\" and its id-url is \"http://arxiv.org/abs/1506.06430v2\".\n",
      "Retriever: I am going to call  download_pdf\n",
      "Tool_executor: I am going to executedownload_pdfwith{'id': '1506.06430v2', 'title': 'Interpolating_Distance_Optimal_Transport'}\n",
      "Retriever:I am reporting back to the arxiv_receptionist withThe paper with the title 'An Interpolating Distance between Optimal Transport and Fisher-Rao' has successfully been downloaded.\n",
      "Reporting to receptionist\n",
      "Receptionist: The following has been forwarded to the arxiv_retriever:  Now, let's move on to the second query.\n",
      "\n",
      "Here's the query:\n",
      "\n",
      "`\"Unbalanced optimal transport Dynamic and Kantorovich formulations\"`\n",
      "\n",
      "Please wait for the response.\n",
      "Retriever: I am going to call  get_id_from_url\n",
      "Tool_executor: I am going to executeget_id_from_urlwith{'url': 'https://export.arxiv.org/api/query?search_query=Unbalanced+optimal+transport+Dynamic+and+Kantorovich+formulations&max_results=5'}\n",
      "Scraper: I got the following paperThe most relevant arXiv paper to the query is \"Unbalanced Optimal Transport: Dynamic and Kantorovich Formulation\" and its id-url is \"http://arxiv.org/abs/1508.05216v3\".\n",
      "Retriever: I am going to call  download_pdf\n",
      "Tool_executor: I am going to executedownload_pdfwith{'id': '1508.05216', 'title': 'Unbalanced_Optimal_Transport'}\n",
      "Retriever:I am reporting back to the arxiv_receptionist withThe paper with the title 'Unbalanced Optimal Transport: Dynamic and Kantorovich Formulation' has sucessfully been downloaded.\n",
      "Reporting to receptionist\n",
      "Receptionist**Final Report:**\n",
      "\n",
      "The following papers were successfully retrieved:\n",
      "\n",
      "1. 'An Interpolating Distance between Optimal Transport and Fisher-Rao'\n",
      "2. 'Unbalanced Optimal Transport: Dynamic and Kantorovich Formulation'\n",
      "\n",
      "We are done.\n",
      "**Final Report:**\n",
      "\n",
      "The following papers were successfully retrieved:\n",
      "\n",
      "1. 'An Interpolating Distance between Optimal Transport and Fisher-Rao'\n",
      "2. 'Unbalanced Optimal Transport: Dynamic and Kantorovich Formulation'\n",
      "\n",
      "We are done.\n"
     ]
    }
   ],
   "source": [
    "### This is a multiagent workflow. Its purpose is to retrieve a collection of papers from arxiv.\n",
    "### The input is a 'list' (in the sense of everyday speech) with each element being the name or keywords around the paper (check the bib file).\n",
    "### Under the hood it searches for the most relevant paper and downlads it in the pdf folder.\n",
    "### In the end you get a report of the papers that were retrieved.\n",
    "### This needs an OpenAI API key to work. There are ways around it, but you need to use a Chat method that uses tools. \n",
    "### You can try your own bibliography here. example bib={1. life of brian, 2. death rebearth 3.  time  illustion wondering face }\n",
    "\n",
    "input={\"receptionist_retriever_history\":[HumanMessage(content=\"\")],\n",
    "    \"last_action_outcome\":[HumanMessage(content=\"\")],\n",
    "    \"metadata\":HumanMessage(content=\" \"),\n",
    "    \"article_keywords\":HumanMessage(content=\" \"),\n",
    "    \"title_of_retrieved_paper\":HumanMessage(content=\" \"),\n",
    "    \"should_I_clean\": False}\n",
    "input[\"receptionist_retriever_history\"][0]=HumanMessage(content=\"Please fetch me the following papers:\" + \"1. An interpolating distance between optimal transport and Fisher-Rao metrics , 2 Unbalanced optimal transport: Dynamic and Kantorovich formulations_?\")\n",
    "\n",
    "### Here You can set different agents to staff the workflow. The default is arxiv_retriever_workflow(retrieval_model=ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0), \n",
    "### cleaner_model=ChatNVIDIA(model=\"meta/llama3-70b-instruct\"), receptionist_model=ChatNVIDIA(model=\"meta/llama3-70b-instruct\"))\n",
    "### the retrieval agents needs tools and ChatNVIDIA is still in development. \n",
    "\n",
    "retrieve_app=ArxivRetrievalWorkflow()\n",
    "retrieve_app=retrieve_app.create_workflow()\n",
    "retrieve_app=retrieve_app.compile()\n",
    "state=retrieve_app.invoke(input,{\"recursion_limit\": 100})    \n",
    "print(state[\"receptionist_retriever_history\"][-1].content)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "ArxivRetrievalTool=ArxivRetrievalToolClass()\n",
    "ArxivRetrievalTool=StructuredTool(name=\"ArxivRetrievalTool\",func=ArxivRetrievalTool.retrieve_bib,args_schema=ArxivRetrievalInput,\n",
    "                           description=ArxivRetrievalTool.description)\n",
    "ArxivRetrievalTool.invoke(\"1.Creatine for gains, 2. Is time relative or relativety had its time\")\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### It takes the name of a pdf located in the folder files\\pdfs as inpup and creates two markdowns, one with mupdf and one with nougat.\n",
    "pdf_to_markdown.invoke(\"Li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This tool takes two version of the same file and creates a new one that actually has the best of both worlds\n",
    "### As it is, it just fixes an issue with citation format with nougat by leveraging the ocr one gets from mupdf\n",
    "###(which is lower quality but with the right format). With a little bit of prompting tweeking it can get more\n",
    "### general tasks of this nature. This tool needs an embeding mechanism as well. The reason is to locate the pages\n",
    "### in the second file that correspond to the pages in the first.\n",
    "\n",
    "ocr_enhancer_app=OcrEnchancingWorkflow()\n",
    "ocr_enhancer_app=ocr_enhancer_app.create_workflow()\n",
    "ocr_enhancer_app=ocr_enhancer_app.compile()\n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\"), \"supporting_text_filename\": HumanMessage(content=\"mu_Li\")}\n",
    "state=ocr_enhancer_app.invoke(input)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "OcrEnhancingTool=OcrEnhancingToolClass()\n",
    "OcrEnhancingTool=StructuredTool(name=\"OcrEnhancingTool\",func=OcrEnhancingTool.ocr_enhance,args_schema=OcrEnhancingInput)\n",
    "OcrEnhancingTool.invoke({\"main_text_filename\": \"Li\", \"supporting_text_filename\": \"mu_Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The idea of this chain/tool is to remove the proofs from a paper so it will be easier to make a summary out of it. \n",
    "### The tool uses two chains under the hood. The first stamps the pages of the text that continue a proof from the previous page.\n",
    "### The idea was to help the LLM a bit to recognize proofs. The second LLM is doint the removal.\n",
    "### From all the modules I created, this was the most unsucessful one. Even with strong LLMs like GPT-4o and Opus, I had partial results.\n",
    "### I welcome anyone who can improve the prompt for this tool.\n",
    "proof_remover_app=ProofRemovingWorkflow()\n",
    "proof_remover_app=proof_remover_app.create_workflow()\n",
    "proof_remover_app=proof_remover_app.compile()\n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\"),\"file\":[\"\"],\"report\":HumanMessage(content=\"\")}\n",
    "state=proof_remover_app.invoke(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [02:44<00:00,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proofs were remove and the resulted file is named Li_without_proofs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The proofs were remove and the resulted file is named Li_without_proofs'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "ProofRemoverTool=ProofRemovalToolClass()\n",
    "ProofRemoverTool=StructuredTool(name=\"ProofRemovalTool\",func=ProofRemoverTool.remove_proof,args_schema=ProofRemovalInput)\n",
    "ProofRemoverTool.invoke({\"main_text_filename\": \"Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This tool takes a text found in the folder files/markdowns and creates a set of keywords and a summary. in a form of a string and extracts the keywords and summary.\n",
    "### It is preferable to use a file that doesnt contain proofs because it produces a better summary. \n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\"),\n",
    "           \"report\":HumanMessage(content=\"\"),}\n",
    "keyword_and_summary_app=KeywordAndSummaryWorkflow()\n",
    "keyword_and_summary_app=keyword_and_summary_app.create_workflow()\n",
    "keyword_and_summary_app=keyword_and_summary_app.compile()\n",
    "state=keyword_and_summary_app.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword_and_summary in progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 11/34 [01:33<03:50, 10.00s/it]"
     ]
    }
   ],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "KeywordAndSummaryTool=KeywordAndSummaryToolClass()\n",
    "KeywordAndSummaryTool=StructuredTool(name=\"KeywordAndSummaryTool\",func=KeywordAndSummaryTool.get_keyword_and_summary,args_schema=KeywordSummaryInput)\n",
    "KeywordAndSummaryTool.invoke({\"main_text_filename\": \"Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "C:\\Users\\wias\\AppData\\Local\\Temp\\ipykernel_9740\\2542359041.py:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "  input={\"keywords_and_summary_filename\": HumanMessage(content=\"markdowns\\Li_keyword_and_summary\"), \"target_language\":HumanMessage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation of Li in progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/34 [00:35<09:21, 17.56s/it]"
     ]
    }
   ],
   "source": [
    "input={\"keywords_and_summary_filename\": HumanMessage(content=\"Li_keyword_and_summary\"), \"target_language\":HumanMessage\n",
    "(content=\"en\"),\"main_text_filename\": HumanMessage(content=\"Li\"), \"report\":HumanMessage}\n",
    "\n",
    "translation_app=TranslationWorkflow()\n",
    "translation_app=translation_app.create_workflow()\n",
    "translation_app=translation_app.compile()\n",
    "state=translation_app.invoke(input)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: The keyword_and_summary file does not exist. Assuming keyword_and_summary is blank.\n",
      "Translation of Li in progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 19/34 [02:51<04:08, 16.54s/it]"
     ]
    }
   ],
   "source": [
    "TranslationTool =TranslationToolClass()\n",
    "    \n",
    "TranslationTool=StructuredTool(name=\"TranslationTool\",func=TranslationTool.translate_file,args_schema=TranslatorInput,\n",
    "                           description=TranslationTool.description)\n",
    "TranslationTool.invoke(input={\"keywords_and_summary_filename\":\"\",\"target_language\":\"en\",\"main_text_filename\":\"Li\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
