{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\artno\\anaconda3\\envs\\nvidia\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from simple_workflows import *\n",
    "from simple_tools import *\n",
    "from workflows_as_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receptionist: The following has been forwarded to the arxiv_retriever:  Query: mitochondria small\n",
      "Retriever: I am going to call  get_id_from_url\n",
      "Tool_executor: I am going to executeget_id_from_urlwith{'url': 'https://export.arxiv.org/api/query?search_query=Query:mitochondria+small&max_results=5'}\n",
      "Scraper: I got the following paperThe most relevant arXiv paper to the query is \"Small Generalized Breathers with Exponentially Small Tails for Klein-Gordon Equations\" and its id-url is \"http://arxiv.org/abs/1307.6387v1\".\n",
      "Retriever: I am going to call  download_pdf\n",
      "Tool_executor: I am going to executedownload_pdfwith{'id': '1307.6387v1', 'title': 'small_breathers_Klein_Gordon'}\n",
      "Retriever:I am reporting back to the arxiv_receptionist withThe paper with the title 'Small Generalized Breathers with Exponentially Small Tails for Klein-Gordon Equations' has successfully been downloaded.\n",
      "Reporting to receptionist\n",
      "Receptionist: The following has been forwarded to the arxiv_retriever:  Query: life short\n",
      "Retriever: I am going to call  get_id_from_url\n",
      "Tool_executor: I am going to executeget_id_from_urlwith{'url': 'https://export.arxiv.org/api/query?search_query=Query+life+short&max_results=5'}\n",
      "Scraper: I got the following paperThe most relevant arXiv paper to the query is \"Long Concept Query on Conceptual Taxonomies\" and its id-url is \"http://arxiv.org/abs/1511.09009v1\".\n",
      "Retriever: I am going to call  download_pdf\n",
      "Tool_executor: I am going to executedownload_pdfwith{'id': '1511.09009v1', 'title': 'Long_Concept_Query'}\n",
      "Retriever:I am reporting back to the arxiv_receptionist withThe paper with the title 'Long Concept Query on Conceptual Taxonomies' has successfully been downloaded.\n",
      "Reporting to receptionist\n",
      "ReceptionistReport:\n",
      "The query 'mitochondria small' has been fulfilled, but the paper 'Small Generalized Breathers with Exponentially Small Tails for Klein-Gordon Equations' doesn't seem to fit the query.\n",
      "\n",
      "The query 'life short' has been fulfilled, but the paper 'Long Concept Query on Conceptual Taxonomies' doesn't seem to fit the query.\n",
      "\n",
      "We are done.\n",
      "Report:\n",
      "The query 'mitochondria small' has been fulfilled, but the paper 'Small Generalized Breathers with Exponentially Small Tails for Klein-Gordon Equations' doesn't seem to fit the query.\n",
      "\n",
      "The query 'life short' has been fulfilled, but the paper 'Long Concept Query on Conceptual Taxonomies' doesn't seem to fit the query.\n",
      "\n",
      "We are done.\n"
     ]
    }
   ],
   "source": [
    "### This is a multiagent workflow. Its purpose is to retrieve a collection of papers from arxiv.\n",
    "### The input is a 'list' (in the sense of everyday speech) with each element being the name or keywords around the paper (check the bib file).\n",
    "### Under the hood it searches for the most relevant paper and downlads it in the pdf folder.\n",
    "### In the end you get a report of the papers that were retrieved.\n",
    "### This needs an OpenAI API key to work. There are ways around it, but you need to use a Chat method that uses tools. \n",
    "### You can try your own bibliography here. example bib={1. life of brian, 2. death rebearth 3.  time  illustion wondering face }\n",
    "\n",
    "input={\"receptionist_retriever_history\":[HumanMessage(content=\"\")],\n",
    "    \"last_action_outcome\":[HumanMessage(content=\"\")],\n",
    "    \"metadata\":HumanMessage(content=\" \"),\n",
    "    \"article_keywords\":HumanMessage(content=\" \"),\n",
    "    \"title_of_retrieved_paper\":HumanMessage(content=\" \"),\n",
    "    \"should_I_clean\": False}\n",
    "input[\"receptionist_retriever_history\"][0]=HumanMessage(content=\"Please fetch me the following papers:\" + \"1 mitochondira are really small? , 2 is life really short?\")\n",
    "\n",
    "### Here You can set different agents to staff the workflow. The default is arxiv_retriever_workflow(retrieval_model=ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0), \n",
    "### cleaner_model=ChatNVIDIA(model=\"meta/llama3-70b-instruct\"), receptionist_model=ChatNVIDIA(model=\"meta/llama3-70b-instruct\"))\n",
    "### the retrieval agents needs tools and ChatNVIDIA is still in development. \n",
    "\n",
    "retrieve_app=ArxivRetrievalWorkflow()\n",
    "retrieve_app=retrieve_app.create_workflow()\n",
    "retrieve_app=retrieve_app.compile()\n",
    "state=retrieve_app.invoke(input,{\"recursion_limit\": 100})    \n",
    "print(state[\"receptionist_retriever_history\"][-1].content)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x000002C6E4D529F0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002C6E52C6E70> openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000002C6E4D529F0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002C6E52C6E70> openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "Receptionist: The following has been forwarded to the arxiv_retriever:  creatine gains\n",
      "Retriever: I am going to call  get_id_from_url\n",
      "Tool_executor: I am going to executeget_id_from_urlwith{'url': 'https://export.arxiv.org/api/query?search_query=creatine+gains&max_results=5'}\n",
      "Scraper: I got the following paperThe most relevant arXiv paper to the query \"creatine gains\" is \"Investigating myocardial infarction and its effects in patients with urgent medical problems using advanced data mining tools\" and its id-url is \"http://arxiv.org/abs/2112.07890v1\".\n",
      "Retriever: I am going to call  download_pdf\n",
      "Tool_executor: I am going to executedownload_pdfwith{'id': '2112.07890v1', 'title': 'Myocardial_infarction_effects'}\n",
      "Retriever:I am reporting back to the arxiv_receptionist withThe paper with the title 'Investigating myocardial infarction and its effects in patients with urgent medical problems using advanced data mining tools' has successfully been downloaded.\n",
      "Reporting to receptionist\n",
      "Receptionist: The following has been forwarded to the arxiv_retriever:  is time relative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get info from https://api.smith.langchain.com: LangSmithConnectionError('Connection error caused failure to GET /info  in LangSmith API. Please confirm your internet connection.. ConnectTimeout(MaxRetryError(\"HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /info (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002C6E49E9EB0>, \\'Connection to api.smith.langchain.com timed out. (connect timeout=10.0)\\'))\"))')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever: I am going to call  get_id_from_url\n",
      "Tool_executor: I am going to executeget_id_from_urlwith{'url': 'https://export.arxiv.org/api/query?search_query=time+relative&max_results=5'}\n",
      "Scraper: I got the following paperThe most relevant arXiv paper to the query \"is time relative\" is \"Three levels of understanding physical relativity: Galileo's relativity, Up-to-date Galileo's relativity and Einstein's relativity: A historical survey\" and its id-url is \"http://arxiv.org/abs/physics/0409121v1\".\n",
      "Retriever: I am going to call  download_pdf\n",
      "Tool_executor: I am going to executedownload_pdfwith{'id': 'physics/0409121v1', 'title': 'physical_relativity'}\n",
      "Retriever:I am reporting back to the arxiv_receptionist withThe paper with the title 'Three levels of understanding physical relativity: Galileo's relativity, Up-to-date Galileo's relativity and Einstein's relativity: A historical survey' has successfully been downloaded.\n",
      "Reporting to receptionist\n",
      "ReceptionistReport:\n",
      "\n",
      "* For the query 'creatine gains', a paper was downloaded but it doesn't seem to fit the query.\n",
      "* For the query 'is time relative', a paper was successfully downloaded.\n",
      "\n",
      "We are done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Report:\\n\\n* For the query 'creatine gains', a paper was downloaded but it doesn't seem to fit the query.\\n* For the query 'is time relative', a paper was successfully downloaded.\\n\\nWe are done\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "ArxivRetrievalTool=ArxivRetrievalToolClass()\n",
    "ArxivRetrievalTool=StructuredTool(name=\"ArxivRetrievalTool\",func=ArxivRetrievalTool.retrieve_bib,args_schema=ArxivRetrievalInput,\n",
    "                           description=ArxivRetrievalTool.description)\n",
    "ArxivRetrievalTool.invoke(\"1.Creatine for gains, 2. Is time relative or relativety had its time\")\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the PDF with mupdf...\n",
      "Processing the PDF with nugat...\n",
      "FileLiconverted successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FileLiconverted successfully'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### It takes the name of a pdf located in the folder files\\pdfs as inpup and creates two markdowns, one with mupdf and one with nougat.\n",
    "pdf_to_markdown.invoke(\"Li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [09:31<00:00, 16.80s/it]\n"
     ]
    }
   ],
   "source": [
    "### This tool takes two version of the same file and creates a new one that actually has the best of both worlds\n",
    "### As it is, it just fixes an issue with citation format with nougat by leveraging the ocr one gets from mupdf\n",
    "###(which is lower quality but with the right format). With a little bit of prompting tweeking it can get more\n",
    "### general tasks of this nature. This tool needs an embeding mechanism as well. The reason is to locate the pages\n",
    "### in the second file that correspond to the pages in the first.\n",
    "\n",
    "ocr_enhancer_app=OcrEnchancingWorkflow()\n",
    "ocr_enhancer_app=ocr_enhancer_app.create_workflow()\n",
    "ocr_enhancer_app=ocr_enhancer_app.compile()\n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\"), \"supporting_text_filename\": HumanMessage(content=\"mu_Li\")}\n",
    "state=ocr_enhancer_app.invoke(input)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [11:02<00:00, 19.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "OcrEnhancingTool=OcrEnhancingToolClass()\n",
    "OcrEnhancingTool=StructuredTool(name=\"OcrEnhancingTool\",func=OcrEnhancingTool.ocr_enhance,args_schema=OcrEnhancingInput)\n",
    "OcrEnhancingTool.invoke({\"main_text_filename\": \"Li\", \"supporting_text_filename\": \"mu_Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The idea of this chain/tool is to remove the proofs from a paper so it will be easier to make a summary out of it. \n",
    "### The tool uses two chains under the hood. The first stamps the pages of the text that continue a proof from the previous page.\n",
    "### The idea was to help the LLM a bit to recognize proofs. The second LLM is doint the removal.\n",
    "### From all the modules I created, this was the most unsucessful one. Even with strong LLMs like GPT-4o and Opus, I had partial results.\n",
    "### I welcome anyone who can improve the prompt for this tool.\n",
    "proof_remover_app=ProofReamovingWorkflow()\n",
    "proof_remover_app=proof_remover_app.create_workflow()\n",
    "proof_remover_app=proof_remover_app.compile()\n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\")}\n",
    "state=proof_remover_app.invoke(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "ProofRemoverTool=ProofRemovalToolClass()\n",
    "ProofRemoverTool=StructuredTool(name=\"ProofRemoverTool\",func=ProofRemoverTool.remove_proof,args_schema=ProofRemovalInput)\n",
    "ProofRemoverTool.invoke({\"main_text_filename\": \"Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This tool takes a text found in the folder files/markdowns and creates a set of keywords and a summary. in a form of a string and extracts the keywords and summary.\n",
    "### It is preferable to use a file that doesnt contain proofs because it produces a better summary. \n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\"),\n",
    "           \"report\":HumanMessage(content=\"\"),}\n",
    "keyword_and_summary_app=KeywordAndSummaryWorkflow()\n",
    "keyword_and_summary_app=keyword_and_summary_app.create_workflow()\n",
    "keyword_and_summary_app=keyword_and_summary_app.compile()\n",
    "state=keyword_and_summary_app.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "KeywordAndSummaryTool=KeywordAndSummaryToolClass()\n",
    "KeywordAndSummaryTool=StructuredTool(name=\"KeywordAndSummaryTool\",func=KeywordAndSummaryTool.get_keyword_and_summary,args_schema=KeywordSummaryInput)\n",
    "KeywordAndSummaryTool.invoke({\"main_text_filename\": \"Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input={\"keywords_and_summary_filename\": HumanMessage(content=\"markdowns\\Li_keyword_and_summary\"), \"target_language\":HumanMessage\n",
    "(content=\"en\"),\"main_text_filename\": HumanMessage(content=\"Li\"), \"report\":HumanMessage}\n",
    "\n",
    "translation_app=TranslationWorkflow()\n",
    "translation_app=translation_app.create_workflow()\n",
    "translation_app=translation_app.compile()\n",
    "state=translation_app.invoke(input)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: The keyword_and_summary file does not exist. Assuming keyword_and_summary is blank.\n",
      "Translation of Li in progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 19/34 [02:51<04:08, 16.54s/it]"
     ]
    }
   ],
   "source": [
    "TranslationTool =TranslationToolClass()\n",
    "    \n",
    "TranslationTool=StructuredTool(name=\"TranslationTool\",func=TranslationTool.translate_file,args_schema=TranslatorInput,\n",
    "                           description=TranslationTool.description)\n",
    "tools =  [TranslationTool, pdf_to_markdown]\n",
    "tool_executor=ToolExecutor(tools)\n",
    "input={\"keywords_and_summary_filename\":\"\",\"target_language\":\"en\",\"main_text_filename\":\"Li\"}\n",
    "Invocation=ToolInvocation(tool=\"TranslationTool\", tool_input=input)\n",
    "TranslationTool.invoke(input={\"keywords_and_summary_filename\":\"\",\"target_language\":\"en\",\"main_text_filename\":\"Li\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
